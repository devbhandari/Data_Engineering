{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ca4735a-aaa6-459a-afe9-fcab49e20717",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>name</th><th>courses</th></tr></thead><tbody><tr><td>1</td><td>ram</td><td>List(Maths, Hindi)</td></tr><tr><td>2</td><td>Shyam</td><td>List(Science, Hindi)</td></tr><tr><td>3</td><td>Mohan</td><td>List(English, Hindi)</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "ram",
         [
          "Maths",
          "Hindi"
         ]
        ],
        [
         2,
         "Shyam",
         [
          "Science",
          "Hindi"
         ]
        ],
        [
         3,
         "Mohan",
         [
          "English",
          "Hindi"
         ]
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "courses",
         "type": "{\"type\":\"array\",\"elementType\":\"string\",\"containsNull\":true}"
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- id: long (nullable = true)\n |-- name: string (nullable = true)\n |-- courses: array (nullable = true)\n |    |-- element: string (containsNull = true)\n\n"
     ]
    }
   ],
   "source": [
    "# import pyspark.sql.types\n",
    "# from pyspark.sql import SparkSession\n",
    "data =[(1,'ram',['Maths','Hindi']),(2,'Shyam',['Science','Hindi']),(3,'Mohan',['English','Hindi'])]\n",
    "sch=['id','name','courses']\n",
    "df=spark.createDataFrame(data,sch)\n",
    "display(df)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "891b4ef4-a456-4afb-8c8d-16badbf22752",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----------------+-------+\n| id| name|         courses| skills|\n+---+-----+----------------+-------+\n|  1|  ram|  [Maths, Hindi]|  Maths|\n|  1|  ram|  [Maths, Hindi]|  Hindi|\n|  2|Shyam|[Science, Hindi]|Science|\n|  2|Shyam|[Science, Hindi]|  Hindi|\n|  3|Mohan|[English, Hindi]|English|\n|  3|Mohan|[English, Hindi]|  Hindi|\n+---+-----+----------------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "# use explode functions to create multiple rows for array columns\n",
    "\n",
    "from pyspark.sql.functions import explode,col\n",
    "df1= df.withColumn(\"skills\",explode(col=\"courses\"))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e657b270-84cd-4c92-b4a2-a5e82063a22c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>name</th><th>courses</th></tr></thead><tbody><tr><td>1</td><td>ram</td><td>Maths,Hindi</td></tr><tr><td>2</td><td>Shyam</td><td>Science,Hindi</td></tr><tr><td>3</td><td>Mohan</td><td>English,Hindi</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "ram",
         "Maths,Hindi"
        ],
        [
         2,
         "Shyam",
         "Science,Hindi"
        ],
        [
         3,
         "Mohan",
         "English,Hindi"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "courses",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>name</th><th>courses</th><th>skills</th></tr></thead><tbody><tr><td>1</td><td>ram</td><td>Maths,Hindi</td><td>List(Maths, Hindi)</td></tr><tr><td>2</td><td>Shyam</td><td>Science,Hindi</td><td>List(Science, Hindi)</td></tr><tr><td>3</td><td>Mohan</td><td>English,Hindi</td><td>List(English, Hindi)</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "ram",
         "Maths,Hindi",
         [
          "Maths",
          "Hindi"
         ]
        ],
        [
         2,
         "Shyam",
         "Science,Hindi",
         [
          "Science",
          "Hindi"
         ]
        ],
        [
         3,
         "Mohan",
         "English,Hindi",
         [
          "English",
          "Hindi"
         ]
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "courses",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "skills",
         "type": "{\"type\":\"array\",\"elementType\":\"string\",\"containsNull\":false}"
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Split function to split data on the basis of delimiter and return an array\n",
    "from pyspark.sql.functions import split,col\n",
    "data =[(1,'ram','Maths,Hindi'),(2,'Shyam','Science,Hindi'),(3,'Mohan','English,Hindi')]\n",
    "sch=['id','name','courses']\n",
    "tempdf=spark.createDataFrame(data=data,schema=sch)\n",
    "display(tempdf)\n",
    "splt_df=tempdf.withColumn(\"skills\",split('courses',','))\n",
    "display(splt_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b6c964e-9a9e-441a-8ad2-b6527c33f8b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# array_contains() : this function to check if an array column contains a specific value or not\n",
    "# - returns null if array is null, false -- if doesnt contain that value else true "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a7ddc42-dad1-4575-86b5-c18238f906b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>name</th><th>courses</th><th>HasJavaSkill</th></tr></thead><tbody><tr><td>1</td><td>ram</td><td>List(Maths, Hindi)</td><td>false</td></tr><tr><td>2</td><td>Shyam</td><td>List(Java, Hindi)</td><td>true</td></tr><tr><td>3</td><td>Mohan</td><td>List(English, Hindi)</td><td>false</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "ram",
         [
          "Maths",
          "Hindi"
         ],
         false
        ],
        [
         2,
         "Shyam",
         [
          "Java",
          "Hindi"
         ],
         true
        ],
        [
         3,
         "Mohan",
         [
          "English",
          "Hindi"
         ],
         false
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "courses",
         "type": "{\"type\":\"array\",\"elementType\":\"string\",\"containsNull\":true}"
        },
        {
         "metadata": "{}",
         "name": "HasJavaSkill",
         "type": "\"boolean\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import array_contains,col\n",
    "data =[(1,'ram',['Maths','Hindi']),(2,'Shyam',['Java','Hindi']),(3,'Mohan',['English','Hindi'])]\n",
    "sch=['id','name','courses']\n",
    "df=spark.createDataFrame(data,sch)\n",
    "# display(df)\n",
    "df1 = df.withColumn(\"HasJavaSkill\",array_contains(col('courses'),'Java'))\n",
    "display(df1)\n",
    "\n",
    "#  notes be mindful that it is cases sensitive while comparing the values. it will give output false if we check against JAVA,JaVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a10be952-106e-4d1c-a09d-80d30c678d3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method withColumn in module pyspark.sql.dataframe:\n\nwithColumn(colName: str, col: pyspark.sql.column.Column) -> 'DataFrame' method of pyspark.sql.dataframe.DataFrame instance\n    Returns a new :class:`DataFrame` by adding a column or replacing the\n    existing column that has the same name.\n    \n    The column expression must be an expression over this :class:`DataFrame`; attempting to add\n    a column from some other :class:`DataFrame` will raise an error.\n    \n    .. versionadded:: 1.3.0\n    \n    .. versionchanged:: 3.4.0\n        Support Spark Connect.\n    \n    Parameters\n    ----------\n    colName : str\n        string, name of the new column.\n    col : :class:`Column`\n        a :class:`Column` expression for the new column.\n    \n    Returns\n    -------\n    :class:`DataFrame`\n        DataFrame with new or replaced column.\n    \n    Notes\n    -----\n    This method introduces a projection internally. Therefore, calling it multiple\n    times, for instance, via loops in order to add multiple columns can generate big\n    plans which can cause performance issues and even `StackOverflowException`.\n    To avoid this, use :func:`select` with multiple columns at once.\n    \n    Examples\n    --------\n    >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n    >>> df.withColumn('age2', df.age + 2).show()\n    +---+-----+----+\n    |age| name|age2|\n    +---+-----+----+\n    |  2|Alice|   4|\n    |  5|  Bob|   7|\n    +---+-----+----+\n\n"
     ]
    }
   ],
   "source": [
    "help(df.withColumn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29ee9bcd-82a6-4fb9-b77a-cc5a4f942730",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>name</th><th>Address</th></tr></thead><tbody><tr><td>1</td><td>ram</td><td>Map(country -> INDIA, state -> rajasthan)</td></tr><tr><td>2</td><td>Scott</td><td>Map(country -> USA, state -> newyork)</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "ram",
         {
          "country": "INDIA",
          "state": "rajasthan"
         }
        ],
        [
         2,
         "Scott",
         {
          "country": "USA",
          "state": "newyork"
         }
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Address",
         "type": "{\"type\":\"map\",\"keyType\":\"string\",\"valueType\":\"string\",\"valueContainsNull\":true}"
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- id: integer (nullable = true)\n |-- name: string (nullable = true)\n |-- Address: map (nullable = true)\n |    |-- key: string\n |    |-- value: string (valueContainsNull = true)\n\n"
     ]
    }
   ],
   "source": [
    "## Video 15. Map Function in Pyspark\n",
    "# map function is like dictionary in python, where maptype will represent the values in key and value pair\n",
    "from pyspark.sql.types import StructType,StructField,MapType,StringType,IntegerType\n",
    "sample_data=[(1,'ram',{'country':'INDIA','state':'rajasthan'}),\n",
    "             (2,'Scott',{'country':'USA','state':'newyork'})\n",
    "             ]\n",
    "# sch=['id','name','address']\n",
    "sch = StructType([ \\\n",
    "        StructField('id',dataType=IntegerType()), \\\n",
    "        StructField('name',dataType=StringType()), \\\n",
    "        StructField('Address',MapType(keyType=StringType(),valueType=StringType()))\n",
    "\n",
    "])\n",
    "df=spark.createDataFrame(sample_data,sch)\n",
    "display(df)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b84e4224-fc55-4302-84e0-6c4f2935ce59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-1342540448314897>:22\u001B[0m\n",
       "\u001B[1;32m     19\u001B[0m df \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mcreateDataFrame(data, schema)\n",
       "\u001B[1;32m     21\u001B[0m \u001B[38;5;66;03m# Handle null values in the 'courses' column\u001B[39;00m\n",
       "\u001B[0;32m---> 22\u001B[0m df_cleaned \u001B[38;5;241m=\u001B[39m df\u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcourses\u001B[39m\u001B[38;5;124m\"\u001B[39m, when(col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcourses\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39misNull(), [])\n",
       "\u001B[1;32m     23\u001B[0m                                       \u001B[38;5;241m.\u001B[39motherwise(col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcourses\u001B[39m\u001B[38;5;124m\"\u001B[39m)))\n",
       "\u001B[1;32m     25\u001B[0m \u001B[38;5;66;03m# Show the cleaned DataFrame\u001B[39;00m\n",
       "\u001B[1;32m     26\u001B[0m df_cleaned\u001B[38;5;241m.\u001B[39mshow(truncate\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py:164\u001B[0m, in \u001B[0;36mtry_remote_functions.<locals>.wrapped\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    162\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(functions, f\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m    163\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m--> 164\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/functions.py:3817\u001B[0m, in \u001B[0;36mwhen\u001B[0;34m(condition, value)\u001B[0m\n",
       "\u001B[1;32m   3811\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkTypeError(\n",
       "\u001B[1;32m   3812\u001B[0m         error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNOT_A_COLUMN\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m   3813\u001B[0m         message_parameters\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_name\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcondition\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_type\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mtype\u001B[39m(condition)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m},\n",
       "\u001B[1;32m   3814\u001B[0m     )\n",
       "\u001B[1;32m   3815\u001B[0m v \u001B[38;5;241m=\u001B[39m value\u001B[38;5;241m.\u001B[39m_jc \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(value, Column) \u001B[38;5;28;01melse\u001B[39;00m value\n",
       "\u001B[0;32m-> 3817\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_invoke_function\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mwhen\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcondition\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mv\u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/functions.py:90\u001B[0m, in \u001B[0;36m_invoke_function\u001B[0;34m(name, *args)\u001B[0m\n",
       "\u001B[1;32m     88\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m SparkContext\u001B[38;5;241m.\u001B[39m_active_spark_context \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m     89\u001B[0m jf \u001B[38;5;241m=\u001B[39m _get_jvm_function(name, SparkContext\u001B[38;5;241m.\u001B[39m_active_spark_context)\n",
       "\u001B[0;32m---> 90\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m Column(\u001B[43mjf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:228\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    226\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdeco\u001B[39m(\u001B[38;5;241m*\u001B[39ma: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n",
       "\u001B[1;32m    227\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m--> 228\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    229\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
       "\u001B[1;32m    230\u001B[0m         converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326\u001B[0m, in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n",
       "\u001B[1;32m    324\u001B[0m value \u001B[38;5;241m=\u001B[39m OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[38;5;241m2\u001B[39m:], gateway_client)\n",
       "\u001B[1;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m REFERENCE_TYPE:\n",
       "\u001B[0;32m--> 326\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n",
       "\u001B[1;32m    327\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n",
       "\u001B[1;32m    328\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n",
       "\u001B[1;32m    329\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    330\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n",
       "\u001B[1;32m    331\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n",
       "\u001B[1;32m    332\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n",
       "\n",
       "\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling z:org.apache.spark.sql.functions.when.\n",
       ": org.apache.spark.SparkRuntimeException: [UNSUPPORTED_FEATURE.LITERAL_TYPE] The feature is not supported: Literal for '[]' of class java.util.ArrayList.\n",
       "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.literalTypeUnsupportedError(QueryExecutionErrors.scala:376)\n",
       "\tat org.apache.spark.sql.catalyst.expressions.Literal$.apply(literals.scala:103)\n",
       "\tat org.apache.spark.sql.functions$.lit(functions.scala:128)\n",
       "\tat org.apache.spark.sql.functions$.when(functions.scala:1516)\n",
       "\tat org.apache.spark.sql.functions.when(functions.scala)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:306)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n",
       "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)\nFile \u001B[0;32m<command-1342540448314897>:22\u001B[0m\n\u001B[1;32m     19\u001B[0m df \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mcreateDataFrame(data, schema)\n\u001B[1;32m     21\u001B[0m \u001B[38;5;66;03m# Handle null values in the 'courses' column\u001B[39;00m\n\u001B[0;32m---> 22\u001B[0m df_cleaned \u001B[38;5;241m=\u001B[39m df\u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcourses\u001B[39m\u001B[38;5;124m\"\u001B[39m, when(col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcourses\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39misNull(), [])\n\u001B[1;32m     23\u001B[0m                                       \u001B[38;5;241m.\u001B[39motherwise(col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcourses\u001B[39m\u001B[38;5;124m\"\u001B[39m)))\n\u001B[1;32m     25\u001B[0m \u001B[38;5;66;03m# Show the cleaned DataFrame\u001B[39;00m\n\u001B[1;32m     26\u001B[0m df_cleaned\u001B[38;5;241m.\u001B[39mshow(truncate\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py:164\u001B[0m, in \u001B[0;36mtry_remote_functions.<locals>.wrapped\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    162\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(functions, f\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    163\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 164\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/functions.py:3817\u001B[0m, in \u001B[0;36mwhen\u001B[0;34m(condition, value)\u001B[0m\n\u001B[1;32m   3811\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkTypeError(\n\u001B[1;32m   3812\u001B[0m         error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNOT_A_COLUMN\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   3813\u001B[0m         message_parameters\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_name\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcondition\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_type\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mtype\u001B[39m(condition)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m},\n\u001B[1;32m   3814\u001B[0m     )\n\u001B[1;32m   3815\u001B[0m v \u001B[38;5;241m=\u001B[39m value\u001B[38;5;241m.\u001B[39m_jc \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(value, Column) \u001B[38;5;28;01melse\u001B[39;00m value\n\u001B[0;32m-> 3817\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_invoke_function\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mwhen\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcondition\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mv\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/functions.py:90\u001B[0m, in \u001B[0;36m_invoke_function\u001B[0;34m(name, *args)\u001B[0m\n\u001B[1;32m     88\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m SparkContext\u001B[38;5;241m.\u001B[39m_active_spark_context \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     89\u001B[0m jf \u001B[38;5;241m=\u001B[39m _get_jvm_function(name, SparkContext\u001B[38;5;241m.\u001B[39m_active_spark_context)\n\u001B[0;32m---> 90\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m Column(\u001B[43mjf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m)\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:228\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    226\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdeco\u001B[39m(\u001B[38;5;241m*\u001B[39ma: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[1;32m    227\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 228\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    229\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    230\u001B[0m         converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326\u001B[0m, in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    324\u001B[0m value \u001B[38;5;241m=\u001B[39m OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[38;5;241m2\u001B[39m:], gateway_client)\n\u001B[1;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m REFERENCE_TYPE:\n\u001B[0;32m--> 326\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n\u001B[1;32m    327\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    328\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n\u001B[1;32m    329\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    330\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n\u001B[1;32m    331\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    332\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n\n\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling z:org.apache.spark.sql.functions.when.\n: org.apache.spark.SparkRuntimeException: [UNSUPPORTED_FEATURE.LITERAL_TYPE] The feature is not supported: Literal for '[]' of class java.util.ArrayList.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.literalTypeUnsupportedError(QueryExecutionErrors.scala:376)\n\tat org.apache.spark.sql.catalyst.expressions.Literal$.apply(literals.scala:103)\n\tat org.apache.spark.sql.functions$.lit(functions.scala:128)\n\tat org.apache.spark.sql.functions$.when(functions.scala:1516)\n\tat org.apache.spark.sql.functions.when(functions.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n\tat java.lang.Thread.run(Thread.java:750)\n",
       "errorSummary": "org.apache.spark.SparkRuntimeException: [UNSUPPORTED_FEATURE.LITERAL_TYPE] The feature is not supported: Literal for '[]' of class java.util.ArrayList.",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# handle null values in array (no array, array with missing element etc)\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, array_remove\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Handle Null Values Example\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Sample DataFrame with null values in the 'courses' column\n",
    "data = [\n",
    "    (1, 'Alice', ['Maths', 'Hindi']),\n",
    "    (2, 'Bob', None),\n",
    "    (3, 'Charlie', ['Science', None]),\n",
    "    (4, 'David', ['English', 'Hindi', 'Maths'])\n",
    "]\n",
    "schema = [\"id\", \"name\", \"courses\"]\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Handle null values in the 'courses' column\n",
    "df_cleaned = df.withColumn(\"courses\", when(col(\"courses\").isNull(), [])\n",
    "                                      .otherwise(col(\"courses\")))\n",
    "\n",
    "# Show the cleaned DataFrame\n",
    "df_cleaned.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb55b289-99d3-4002-a586-f2a81f4fbb7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>name</th><th>Address</th><th>Country</th><th>State</th></tr></thead><tbody><tr><td>1</td><td>ram</td><td>Map(country -> INDIA, state -> rajasthan)</td><td>INDIA</td><td>rajasthan</td></tr><tr><td>2</td><td>Scott</td><td>Map(country -> USA, state -> newyork)</td><td>USA</td><td>newyork</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "ram",
         {
          "country": "INDIA",
          "state": "rajasthan"
         },
         "INDIA",
         "rajasthan"
        ],
        [
         2,
         "Scott",
         {
          "country": "USA",
          "state": "newyork"
         },
         "USA",
         "newyork"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Address",
         "type": "{\"type\":\"map\",\"keyType\":\"string\",\"valueType\":\"string\",\"valueContainsNull\":true}"
        },
        {
         "metadata": "{}",
         "name": "Country",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "State",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# to access the value of map in diffrent column\n",
    "df1 = df \\\n",
    "        .withColumn('Country',df.Address['country']\n",
    "            #  'State',df.Address['state']       \n",
    "                    ) \\\n",
    "        .withColumn('State',df.Address.getItem('state'))\n",
    "display(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d55c301-9499-4e71-84c5-1cda591730af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#map_keys and map_values functions provides the keys and values from Dictionary in pyspark.\n",
    "# in above example map_keys will provide country and state however map_values will provide values like india,rajasthan,usa,newyork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b9f9b0a-c258-45ee-b06b-8b9eb20c4555",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ramesh 1000\nrajesh 1000\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>name</th><th>sal</th></tr></thead><tbody><tr><td>raj</td><td>1000</td></tr><tr><td>sartaj</td><td>2000</td></tr><tr><td>Faraz</td><td>3000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "raj",
         1000
        ],
        [
         "sartaj",
         2000
        ],
        [
         "Faraz",
         3000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "sal",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- name: string (nullable = true)\n |-- sal: long (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "# Video17: ROW class in pyspark\n",
    "# In PySpark, Row is a class in the pyspark.sql module that represents a single row of data in a DataFrame. It is similar to a named tuple in Python, where fields can be accessed as attributes or as dictionary keys.\n",
    "# Row objects are used internally by PySpark to store and manipulate data within DataFrames. You can also create Row objects explicitly to construct DataFrames from a list of rows.\n",
    "\n",
    "from pyspark.sql import Row\n",
    "row1=Row('ramesh','1000')\n",
    "# access by index\n",
    "print(row1[0] + ' ' + str(row1[1]))\n",
    "#  access by param name\n",
    "row2=Row(name='rajesh', salary='1000')\n",
    "print(row2.name + ' ' + str(row2.salary))\n",
    "\n",
    "# you can create dataframe using above row function\n",
    "from pyspark.sql import Row\n",
    "row1= Row(name='raj',sal=1000)\n",
    "row2= Row(name='sartaj',sal=2000)\n",
    "row3= Row(name='Faraz',sal=3000)\n",
    "\n",
    "data = [row1,row2,row3]\n",
    "df=spark.createDataFrame(data=data)\n",
    "display(df)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34d6b90c-5674-474b-8435-b3a441d18a99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 18: columns in pyspark\n",
    "# it represent a column in dataframe, you can perfrom various operations on column.\n",
    "# to create a column class object the simplest way is to use lit function from pyspark.sql.functions import lit\n",
    "\n",
    "# from pyspark.sql.functions import lit\n",
    "# col1 = lit('abcd')\n",
    "# print(col1)\n",
    "# print(type(col1))\n",
    "\n",
    "#different ways to fetch the column\n",
    "\n",
    "data =[(1,'ram',['Maths','Hindi']),(2,'Shyam',['Science','Hindi']),(3,'Mohan',['English','Hindi'])]\n",
    "sch=['id','name','courses']\n",
    "df=spark.createDataFrame(data,sch)\n",
    "# display(df)\n",
    "# df.printSchema()\n",
    "\n",
    "# 1st way to fetch column using column name\n",
    "# df1=df.select(df.name).show()\n",
    "# 2nd way to fetch column using index\n",
    "# df2=df.select(df['name']).show()\n",
    "# 3rd way to fetch column using col function\n",
    "# df3=df.select(col('name')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8e96d75-fa9d-473f-a58d-a3a9c231fc80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 19: when and otherwise functions in pyspark\n",
    "from pyspark.sql.functions import when\n",
    "data =[(1,'ram','M',1000),(2,'Shyam','M',2000),(3,'Mohan','',3000),(3,'Mohan','',3000)]\n",
    "sch=['id','name','sex','salary']\n",
    "df=spark.createDataFrame(data,sch)\n",
    "# display(df)\n",
    "# df.printSchema()\n",
    "# df1=df.select(df.id,df.name,\\\n",
    "#               when(df.sex =='M','Male').\\\n",
    "#               when (df.sex =='F','Female').\\\n",
    "#               otherwise('NotMentioned').alias('gender')\n",
    "#               ).show()\n",
    "#20 : alias ,asc,desc,cast,like\n",
    "# alias\n",
    "# df.select(df.id.alias('emp_id'),df.name.alias('emp_name'),df.sex.alias('gender'),df.salary.alias('emp_salary')).show()\n",
    "# asc\n",
    "# df.sort(df.name.asc()).show()\n",
    "# cast\n",
    "# df1=df.select(df.id,df.name,df.sex,df.salary.cast('int'))\n",
    "# df1.printSchema()\n",
    "\n",
    "# like function\n",
    "# df2=df.filter(df.name.like('M%'))\n",
    "# display(df2)\n",
    "\n",
    "#21: filter() and where() in pyspark\n",
    "\n",
    "#filter\n",
    "# display(df)\n",
    "# df.filter(df.sex=='M').show()\n",
    "# df.filter(df.salary > 2000).show()\n",
    "\n",
    "#where :\n",
    "# df.where(df.sex=='M').show()\n",
    "# df.where((df.salary >= 2000) & (df.sex=='M')).show()\n",
    "\n",
    "# 22. distinct() and dropduplicates()\n",
    "# #  Distinct take all columns in consideration for removing duplicates, however dropduplicates take one or more columns in consideration\n",
    "# df.show() # will show 4 rows\n",
    "# df.distinct().show() # will show 3 rows\n",
    "# df.dropDuplicates().show()# will show 3 rows\n",
    "# df.dropDuplicates(['sex']).show()# will show 2 rows\n",
    "\n",
    "#23. orderBy and sort() in pyspark\n",
    "# df.show()\n",
    "# df.sort(df.sex.asc(),df.salary.desc()).show()\n",
    "# df.orderBy(df.sex.asc(),df.salary.desc()).show()\n",
    "\n",
    "#24. Union vs UnionAll\n",
    "# same as sql (to merge 2 or more than 2 dataframes with same schema or structure). to remove duplicates use distinct cz even union will not remove it unlike sql\n",
    "\n",
    "data1=[(1,'ram','delhi',1000),(2,'shyam','jaipur',2000),(3,'Mohan','delhi',3000),(3,'Mohan','delhi',3000)]\n",
    "df1=spark.createDataFrame(data1,['id','name','place','salary'])\n",
    "\n",
    "data2=[(11,'abram','delhi',1000),(12,'Ghanshyam','jaipur',2000),(13,'Mohana','delhi',3000)]\n",
    "df2=spark.createDataFrame(data2,['id','name','place','salary'])\n",
    "\n",
    "# union_merged_df=df1.union(df2)\n",
    "# # display(union_merged_df) #dups are here\n",
    "# unionall_merged_df=df1.union(df2)\n",
    "# # display(unionall_merged_df) #dups are here\n",
    "# unionall_merged_df.distinct().show()\n",
    "# union_merged_df.distinct().show()\n",
    "\n",
    "# 25.Groupby in spark:\n",
    "# same as sql group by\n",
    "# df.show()\n",
    "# df.groupBy('sex').count().show()\n",
    "# df.groupBy('sex').sum('salary').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66b3ac15-fde3-424e-8766-9aa291958a1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 26. groupby and agg() fuynctions in pyspark.\n",
    "# agg()-- to calculate more than one aggregate functions in one go\n",
    "\n",
    "\n",
    "# df.printSchema()\n",
    "# # \n",
    "# # df.groupBy('gender').count().alias('count_of_emp').show() # one agg function at a time\n",
    "\n",
    "# df.groupBy('gender').agg(count('*').alias('count_emp_per_gender'),\\\n",
    "#                         min('salary').alias('min_sal_per_gender'),\\\n",
    "#                         max('salary').alias('max_Sal_pr_gender'),\\\n",
    "#                         sum('salary').alias('sum_Sal_pr_gender')).show()\n",
    "\n",
    "#27: unionByname in pyspark\n",
    "# merge two or more dataframe with different schema(columns) by passing allowmissingcolumns with value as True.\n",
    "\n",
    "# data1=[(1,'ram','delhi',1000),(2,'shyam','jaipur',2000),(3,'Mohan','delhi',3000),(3,'Mohan','delhi',3000)]\n",
    "# df1=spark.createDataFrame(data1,['id','name','place','salary'])\n",
    "\n",
    "# data2=[(11,'abram',1000),(12,'Ghanshyam',2000),(13,'Mohana',3000)]\n",
    "# df2=spark.createDataFrame(data2,['id','name','salary'])\n",
    "\n",
    "# # df1.union(df2).show() #error : UNION can only be performed on inputs with the same number of columns, but the first input has 4 columns and the second input has 3 columns.;\n",
    "# df1.unionByName(df2,allowMissingColumns=True).show()\n",
    "\n",
    "#28: select function in pyspark:similar to select in sql\n",
    "# you may select coulmns using index,columns from list and the nested columns from dataframe\n",
    "# different ways to use the select in pyspark\n",
    "\n",
    "# df.select('id','name').show()\n",
    "# df.select(df.id,df.name).show()\n",
    "# df.select(df['id'],df['name']).show()\n",
    "\n",
    "# from pyspark.sql.functions import col\n",
    "# df.select(col('id'),col('name')).show()\n",
    "\n",
    "# df.select(['id','name','salary']).show()\n",
    "# df.select('*').show()\n",
    "\n",
    "#29 join() in pyspark : similar to sql joins(to join the data from two or more tables/dataframes)\n",
    "# inner :\n",
    "# left:\n",
    "# right\n",
    "# full:\n",
    "\n",
    "# data1=[(1,'ram','delhi',1),(2,'shyam','jaipur',2),(3,'Mohan','delhi',1),(3,'Sohan','delhi',4)]\n",
    "# emp_df=spark.createDataFrame(data1,['id','name','place','dept_id'])\n",
    "\n",
    "# data2=[(1,'IT'),(2,'HR'),(3,'Finance')]\n",
    "# dept_df=spark.createDataFrame(data2,['dept_id','name'])\n",
    "# display(emp_df)\n",
    "# display(dept_df)\n",
    "# # inner join\n",
    "# emp_df.join(dept_df,emp_df.dept_id==dept_df.dept_id,'inner').select('*').show()\n",
    "# # left join\n",
    "# emp_df.join(dept_df,emp_df.dept_id==dept_df.dept_id,'left').select('*').show()\n",
    "# # right join\n",
    "# emp_df.join(dept_df,emp_df.dept_id==dept_df.dept_id,'right').select('*').show()\n",
    "\n",
    "# # full join\n",
    "# emp_df.join(dept_df,emp_df.dept_id==dept_df.dept_id,'full').select('*').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47acb494-45a0-4f45-a184-ac5593d9d7d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n  File \"/databricks/python/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3378, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<command-1461609056163703>\", line 28, in <module>\n    joined_df = emp_alias.join(mgr_alias, emp_alias[\"mgr_id\"] == mgr_alias[\"empid\"], 'inner')\\\n  File \"/databricks/spark/python/pyspark/instrumentation_utils.py\", line 48, in wrapper\n    res = func(*args, **kwargs)\n  File \"/databricks/spark/python/pyspark/sql/dataframe.py\", line 2364, in join\n    jdf = self._jdf.join(other._jdf, on, how)\n  File \"/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1321, in __call__\n    return_value = get_return_value(\n  File \"/databricks/spark/python/pyspark/errors/exceptions.py\", line 234, in deco\n    raise converted from None\npyspark.errors.exceptions.AnalysisException: Column mgr_id#448L are ambiguous. It's probably because you joined several Datasets together, and some of these Datasets are the same. This column points to one of the Datasets but Spark is unable to figure out which one. Please alias the Datasets with different names via `Dataset.as` before joining them, and specify the column using qualified name, e.g. `df.as(\"a\").join(df.as(\"b\"), $\"a.id\" > $\"b.id\")`. You can also set spark.sql.analyzer.failAmbiguousSelfJoin to false to disable this check.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/databricks/python/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 1997, in showtraceback\n    stb = self.InteractiveTB.structured_traceback(\n  File \"/databricks/python/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1112, in structured_traceback\n    return FormattedTB.structured_traceback(\n  File \"/databricks/python/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1006, in structured_traceback\n    return VerboseTB.structured_traceback(\n  File \"/databricks/python/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 859, in structured_traceback\n    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n  File \"/databricks/python/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 812, in format_exception_as_a_whole\n    frames.append(self.format_record(r))\n  File \"/databricks/python/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 730, in format_record\n    result += ''.join(_format_traceback_lines(frame_info.lines, Colors, self.has_colors, lvals))\n  File \"/databricks/python/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n    value = obj.__dict__[self.func.__name__] = self.func(obj)\n  File \"/databricks/python/lib/python3.9/site-packages/stack_data/core.py\", line 698, in lines\n    pieces = self.included_pieces\n  File \"/databricks/python/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n    value = obj.__dict__[self.func.__name__] = self.func(obj)\n  File \"/databricks/python/lib/python3.9/site-packages/stack_data/core.py\", line 649, in included_pieces\n    pos = scope_pieces.index(self.executing_piece)\n  File \"/databricks/python/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n    value = obj.__dict__[self.func.__name__] = self.func(obj)\n  File \"/databricks/python/lib/python3.9/site-packages/stack_data/core.py\", line 628, in executing_piece\n    return only(\n  File \"/databricks/python/lib/python3.9/site-packages/executing/executing.py\", line 164, in only\n    raise NotOneValueFound('Expected one value, found 0')\nexecuting.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: Column mgr_id#448L are ambiguous. It's probably because you joined several Datasets together, and some of these Datasets are the same. This column points to one of the Datasets but Spark is unable to figure out which one. Please alias the Datasets with different names via `Dataset.as` before joining them, and specify the column using qualified name, e.g. `df.as(\"a\").join(df.as(\"b\"), $\"a.id\" > $\"b.id\")`. You can also set spark.sql.analyzer.failAmbiguousSelfJoin to false to disable this check.",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 30 : left semi,left anti, and self join\n",
    "# leftsemi -- similar like inner join but only get rows from left side only dataframe\n",
    "# leftanti -- is opposite to left semi it gets not matching rows from left side dataframe\n",
    "# self join --join data with same dataframe \n",
    "\n",
    "# data1=[(1,'ram','delhi',1),(2,'shyam','jaipur',2),(3,'Mohan','delhi',2),(3,'Sohan','delhi',4)]\n",
    "# emp_df=spark.createDataFrame(data1,['id','name','place','dept_id'])\n",
    "\n",
    "# data2=[(1,'IT'),(2,'HR'),(3,'Finance')]\n",
    "# dept_df=spark.createDataFrame(data2,['dept_id','name'])\n",
    "# display(emp_df)\n",
    "# display(dept_df)\n",
    "# emp_df.join(dept_df,emp_df.dept_id==dept_df.dept_id,'inner').show() #like normal sql inner join columns from both the side of dfs for join criteria\n",
    "# emp_df.join(dept_df,emp_df.dept_id==dept_df.dept_id,'left_semi').show() #same result as above but only left side columns\n",
    "# emp_df.join(dept_df,emp_df.dept_id==dept_df.dept_id,'left_anti').show()# columns from left side only and row which is not matched from left df\n",
    "\n",
    "# SELF JOIN\n",
    "data=[(1,'ram',0),(2,'mukesh',1),(3,'Mohan',2)]\n",
    "sch=['empid','name','mgr_id']\n",
    "emp_df=spark.createDataFrame(data,sch)\n",
    "# display(emp_df)\n",
    "# empdf1=emp_df.alias('emp')\n",
    "# mgrdf2=emp_df.alias('mgr')\n",
    "# tt=empdf1.join(mgrdf2,empdf1.mgr_id==mgrdf2.empid,'inner').show()\n",
    "# tt.show()\n",
    "emp_alias = emp_df.alias('emp')\n",
    "mgr_alias = emp_df.alias('mgr')\n",
    "joined_df = emp_alias.join(mgr_alias, emp_alias[\"mgr_id\"] == mgr_alias[\"empid\"], 'inner')\\\n",
    "            .select(emp_alias[\"empid\"].alias(\"emp_empid\"),\n",
    "                    emp_alias[\"name\"].alias(\"emp_name\"),\n",
    "                    emp_alias[\"mgr_id\"].alias(\"emp_mgr_id\"),\n",
    "                    mgr_alias[\"empid\"].alias(\"mgr_empid\"),\n",
    "                    mgr_alias[\"name\"].alias(\"mgr_name\"),\n",
    "                    mgr_alias[\"mgr_id\"].alias(\"mgr_mgr_id\")\n",
    "            )\n",
    "joined_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3831a7ad-5409-41fe-a08b-19a1d85b89f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-3217848536609598>:19\u001B[0m\n",
       "\u001B[1;32m     15\u001B[0m mgr_alias \u001B[38;5;241m=\u001B[39m emp_df\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmgr\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
       "\u001B[1;32m     17\u001B[0m \u001B[38;5;66;03m# Perform the join\u001B[39;00m\n",
       "\u001B[1;32m     18\u001B[0m joined_df \u001B[38;5;241m=\u001B[39m emp_alias\u001B[38;5;241m.\u001B[39mjoin(mgr_alias\u001B[38;5;241m.\u001B[39mwithColumnRenamed(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mempid\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmgr_empid\u001B[39m\u001B[38;5;124m\"\u001B[39m), \n",
       "\u001B[0;32m---> 19\u001B[0m                            emp_alias[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmgr_id\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m==\u001B[39m mgr_alias[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmgr_empid\u001B[39m\u001B[38;5;124m\"\u001B[39m], \n",
       "\u001B[1;32m     20\u001B[0m                            \u001B[38;5;124m'\u001B[39m\u001B[38;5;124minner\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
       "\u001B[1;32m     22\u001B[0m \u001B[38;5;66;03m# Select the desired columns\u001B[39;00m\n",
       "\u001B[1;32m     23\u001B[0m joined_df \u001B[38;5;241m=\u001B[39m joined_df\u001B[38;5;241m.\u001B[39mselect(\n",
       "\u001B[1;32m     24\u001B[0m     emp_alias[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mempid\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124memp_empid\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n",
       "\u001B[1;32m     25\u001B[0m     emp_alias[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mname\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124memp_name\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m     28\u001B[0m     mgr_alias[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mname\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmgr_name\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m     29\u001B[0m )\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:2918\u001B[0m, in \u001B[0;36mDataFrame.__getitem__\u001B[0;34m(self, item)\u001B[0m\n",
       "\u001B[1;32m   2876\u001B[0m \u001B[38;5;124;03m\"\"\"Returns the column as a :class:`Column`.\u001B[39;00m\n",
       "\u001B[1;32m   2877\u001B[0m \n",
       "\u001B[1;32m   2878\u001B[0m \u001B[38;5;124;03m.. versionadded:: 1.3.0\u001B[39;00m\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m   2915\u001B[0m \u001B[38;5;124;03m+---+----+\u001B[39;00m\n",
       "\u001B[1;32m   2916\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
       "\u001B[1;32m   2917\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(item, \u001B[38;5;28mstr\u001B[39m):\n",
       "\u001B[0;32m-> 2918\u001B[0m     jc \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mitem\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   2919\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m Column(jc)\n",
       "\u001B[1;32m   2920\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(item, Column):\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `mgr_empid` cannot be resolved. Did you mean one of the following? [`empid`, `name`, `mgr_id`]."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\nFile \u001B[0;32m<command-3217848536609598>:19\u001B[0m\n\u001B[1;32m     15\u001B[0m mgr_alias \u001B[38;5;241m=\u001B[39m emp_df\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmgr\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     17\u001B[0m \u001B[38;5;66;03m# Perform the join\u001B[39;00m\n\u001B[1;32m     18\u001B[0m joined_df \u001B[38;5;241m=\u001B[39m emp_alias\u001B[38;5;241m.\u001B[39mjoin(mgr_alias\u001B[38;5;241m.\u001B[39mwithColumnRenamed(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mempid\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmgr_empid\u001B[39m\u001B[38;5;124m\"\u001B[39m), \n\u001B[0;32m---> 19\u001B[0m                            emp_alias[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmgr_id\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m==\u001B[39m mgr_alias[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmgr_empid\u001B[39m\u001B[38;5;124m\"\u001B[39m], \n\u001B[1;32m     20\u001B[0m                            \u001B[38;5;124m'\u001B[39m\u001B[38;5;124minner\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     22\u001B[0m \u001B[38;5;66;03m# Select the desired columns\u001B[39;00m\n\u001B[1;32m     23\u001B[0m joined_df \u001B[38;5;241m=\u001B[39m joined_df\u001B[38;5;241m.\u001B[39mselect(\n\u001B[1;32m     24\u001B[0m     emp_alias[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mempid\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124memp_empid\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m     25\u001B[0m     emp_alias[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mname\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124memp_name\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     28\u001B[0m     mgr_alias[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mname\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmgr_name\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     29\u001B[0m )\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:2918\u001B[0m, in \u001B[0;36mDataFrame.__getitem__\u001B[0;34m(self, item)\u001B[0m\n\u001B[1;32m   2876\u001B[0m \u001B[38;5;124;03m\"\"\"Returns the column as a :class:`Column`.\u001B[39;00m\n\u001B[1;32m   2877\u001B[0m \n\u001B[1;32m   2878\u001B[0m \u001B[38;5;124;03m.. versionadded:: 1.3.0\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   2915\u001B[0m \u001B[38;5;124;03m+---+----+\u001B[39;00m\n\u001B[1;32m   2916\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   2917\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(item, \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m-> 2918\u001B[0m     jc \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mitem\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2919\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m Column(jc)\n\u001B[1;32m   2920\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(item, Column):\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mAnalysisException\u001B[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `mgr_empid` cannot be resolved. Did you mean one of the following? [`empid`, `name`, `mgr_id`].",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `mgr_empid` cannot be resolved. Did you mean one of the following? [`empid`, `name`, `mgr_id`].",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder.appName(\"JoinExample\").getOrCreate()\n",
    "\n",
    "# Sample data and schema\n",
    "data = [(1, 'ram', 0), (2, 'mukesh', 1), (3, 'Mohan', 2)]\n",
    "sch = ['empid', 'name', 'mgr_id']\n",
    "\n",
    "# Create DataFrame\n",
    "emp_df = spark.createDataFrame(data, sch)\n",
    "\n",
    "# Alias the DataFrame\n",
    "emp_alias = emp_df.alias('emp')\n",
    "mgr_alias = emp_df.alias('mgr')\n",
    "\n",
    "# Perform the join\n",
    "joined_df = emp_alias.join(mgr_alias.withColumnRenamed(\"empid\", \"mgr_empid\"), \n",
    "                           emp_alias[\"mgr_id\"] == mgr_alias[\"mgr_empid\"], \n",
    "                           'inner')\n",
    "\n",
    "# Select the desired columns\n",
    "joined_df = joined_df.select(\n",
    "    emp_alias[\"empid\"].alias(\"emp_empid\"),\n",
    "    emp_alias[\"name\"].alias(\"emp_name\"),\n",
    "    emp_alias[\"mgr_id\"].alias(\"emp_mgr_id\"),\n",
    "    mgr_alias[\"empid\"].alias(\"mgr_empid\"),  \n",
    "    mgr_alias[\"name\"].alias(\"mgr_name\")\n",
    ")\n",
    "\n",
    "# Show the result\n",
    "joined_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07539a91-2020-4594-ad72-4038a8aa36df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-3217848536609599>:21\u001B[0m\n",
       "\u001B[1;32m     18\u001B[0m mgr_alias \u001B[38;5;241m=\u001B[39m mgr_alias\u001B[38;5;241m.\u001B[39mwithColumnRenamed(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mempid\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmgr_empid\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mwithColumnRenamed(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mname\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmgr_name\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m     20\u001B[0m \u001B[38;5;66;03m# Perform the join\u001B[39;00m\n",
       "\u001B[0;32m---> 21\u001B[0m joined_df \u001B[38;5;241m=\u001B[39m emp_alias\u001B[38;5;241m.\u001B[39mjoin(mgr_alias, emp_alias[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmgr_id\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m==\u001B[39m mgr_alias[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmgr_empid\u001B[39m\u001B[38;5;124m\"\u001B[39m], \u001B[38;5;124m'\u001B[39m\u001B[38;5;124minner\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
       "\u001B[1;32m     23\u001B[0m \u001B[38;5;66;03m# Select the desired columns\u001B[39;00m\n",
       "\u001B[1;32m     24\u001B[0m joined_df \u001B[38;5;241m=\u001B[39m joined_df\u001B[38;5;241m.\u001B[39mselect(\n",
       "\u001B[1;32m     25\u001B[0m     emp_alias[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mempid\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124memp_empid\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n",
       "\u001B[1;32m     26\u001B[0m     emp_alias[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mname\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124memp_name\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m     29\u001B[0m     mgr_alias[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmgr_name\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
       "\u001B[1;32m     30\u001B[0m )\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:2364\u001B[0m, in \u001B[0;36mDataFrame.join\u001B[0;34m(self, other, on, how)\u001B[0m\n",
       "\u001B[1;32m   2362\u001B[0m         on \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jseq([])\n",
       "\u001B[1;32m   2363\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(how, \u001B[38;5;28mstr\u001B[39m), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhow should be a string\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[0;32m-> 2364\u001B[0m     jdf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43mother\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mon\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhow\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   2365\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(jdf, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparkSession)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: Column mgr_id#576L are ambiguous. It's probably because you joined several Datasets together, and some of these Datasets are the same. This column points to one of the Datasets but Spark is unable to figure out which one. Please alias the Datasets with different names via `Dataset.as` before joining them, and specify the column using qualified name, e.g. `df.as(\"a\").join(df.as(\"b\"), $\"a.id\" > $\"b.id\")`. You can also set spark.sql.analyzer.failAmbiguousSelfJoin to false to disable this check."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\nFile \u001B[0;32m<command-3217848536609599>:21\u001B[0m\n\u001B[1;32m     18\u001B[0m mgr_alias \u001B[38;5;241m=\u001B[39m mgr_alias\u001B[38;5;241m.\u001B[39mwithColumnRenamed(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mempid\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmgr_empid\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mwithColumnRenamed(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mname\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmgr_name\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     20\u001B[0m \u001B[38;5;66;03m# Perform the join\u001B[39;00m\n\u001B[0;32m---> 21\u001B[0m joined_df \u001B[38;5;241m=\u001B[39m emp_alias\u001B[38;5;241m.\u001B[39mjoin(mgr_alias, emp_alias[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmgr_id\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m==\u001B[39m mgr_alias[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmgr_empid\u001B[39m\u001B[38;5;124m\"\u001B[39m], \u001B[38;5;124m'\u001B[39m\u001B[38;5;124minner\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     23\u001B[0m \u001B[38;5;66;03m# Select the desired columns\u001B[39;00m\n\u001B[1;32m     24\u001B[0m joined_df \u001B[38;5;241m=\u001B[39m joined_df\u001B[38;5;241m.\u001B[39mselect(\n\u001B[1;32m     25\u001B[0m     emp_alias[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mempid\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124memp_empid\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m     26\u001B[0m     emp_alias[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mname\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124memp_name\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     29\u001B[0m     mgr_alias[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmgr_name\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m     30\u001B[0m )\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:2364\u001B[0m, in \u001B[0;36mDataFrame.join\u001B[0;34m(self, other, on, how)\u001B[0m\n\u001B[1;32m   2362\u001B[0m         on \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jseq([])\n\u001B[1;32m   2363\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(how, \u001B[38;5;28mstr\u001B[39m), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhow should be a string\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m-> 2364\u001B[0m     jdf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43mother\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mon\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhow\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2365\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(jdf, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparkSession)\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mAnalysisException\u001B[0m: Column mgr_id#576L are ambiguous. It's probably because you joined several Datasets together, and some of these Datasets are the same. This column points to one of the Datasets but Spark is unable to figure out which one. Please alias the Datasets with different names via `Dataset.as` before joining them, and specify the column using qualified name, e.g. `df.as(\"a\").join(df.as(\"b\"), $\"a.id\" > $\"b.id\")`. You can also set spark.sql.analyzer.failAmbiguousSelfJoin to false to disable this check.",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: Column mgr_id#576L are ambiguous. It's probably because you joined several Datasets together, and some of these Datasets are the same. This column points to one of the Datasets but Spark is unable to figure out which one. Please alias the Datasets with different names via `Dataset.as` before joining them, and specify the column using qualified name, e.g. `df.as(\"a\").join(df.as(\"b\"), $\"a.id\" > $\"b.id\")`. You can also set spark.sql.analyzer.failAmbiguousSelfJoin to false to disable this check.",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder.appName(\"JoinExample\").getOrCreate()\n",
    "\n",
    "# Sample data and schema\n",
    "data = [(1, 'ram', 0), (2, 'mukesh', 1), (3, 'Mohan', 2)]\n",
    "sch = ['empid', 'name', 'mgr_id']\n",
    "\n",
    "# Create DataFrame\n",
    "emp_df = spark.createDataFrame(data, sch)\n",
    "\n",
    "# Alias the DataFrame\n",
    "emp_alias = emp_df.alias('emp')\n",
    "mgr_alias = emp_df.alias('mgr')\n",
    "\n",
    "# Rename columns in one of the DataFrames to avoid ambiguity\n",
    "mgr_alias = mgr_alias.withColumnRenamed(\"empid\", \"mgr_empid\").withColumnRenamed(\"name\", \"mgr_name\")\n",
    "\n",
    "# Perform the join\n",
    "joined_df = emp_alias.join(mgr_alias, emp_alias[\"mgr_id\"] == mgr_alias[\"mgr_empid\"], 'inner')\n",
    "\n",
    "# Select the desired columns\n",
    "joined_df = joined_df.select(\n",
    "    emp_alias[\"empid\"].alias(\"emp_empid\"),\n",
    "    emp_alias[\"name\"].alias(\"emp_name\"),\n",
    "    emp_alias[\"mgr_id\"].alias(\"emp_mgr_id\"),\n",
    "    mgr_alias[\"mgr_empid\"],\n",
    "    mgr_alias[\"mgr_name\"]\n",
    ")\n",
    "\n",
    "# Show the result\n",
    "joined_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe2cf2bd-8cff-45b4-85f6-4e256ffbac6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_From_ArrayOnwards",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}